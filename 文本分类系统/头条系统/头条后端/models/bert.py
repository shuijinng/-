# train_bert_fast.py - ä½¿ç”¨æœ¬åœ° BERT æ¨¡å‹å¿«é€Ÿè®­ç»ƒï¼ˆ4060 æ˜¾å¡ä¼˜åŒ–ç‰ˆï¼‰
import os
import torch
from transformers import (
    BertTokenizer,
    BertForSequenceClassification,
    Trainer,
    TrainingArguments,
    set_seed,
)
from torch.utils.data import Dataset
import pandas as pd

# ==================== ç¯å¢ƒè®¾ç½® ====================
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'  # å›½å†…é•œåƒ
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'
os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'

# è®¾ç½®éšæœºç§å­
set_seed(42)

# ==================== è·¯å¾„é…ç½® ====================
BERT_LOCAL_PATH = r"D:\111huiyu\æ…§ä¸\é¢˜\ä½œä¸šé›†åˆ\8æœˆ25æ—¥-å•é›¨é˜³\01.æºä»£ç \å¤´æ¡åç«¯\data\bert_pretrain"
MODEL_DIR = r"D:\111huiyu\æ…§ä¸\é¢˜\ä½œä¸šé›†åˆ\8æœˆ25æ—¥-å•é›¨é˜³\01.æºä»£ç \å¤´æ¡åç«¯\model"
DATA_PATH = r"D:\111huiyu\æ…§ä¸\é¢˜\ä½œä¸šé›†åˆ\8æœˆ25æ—¥-å•é›¨é˜³\01.æºä»£ç \å¤´æ¡åç«¯\data\train_new.csv"
SAVE_PATH = os.path.join(MODEL_DIR, "bert_model")

os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(SAVE_PATH, exist_ok=True)

print(f"ğŸ“ BERT æœ¬åœ°æ¨¡å‹è·¯å¾„: {BERT_LOCAL_PATH}")
print(f"ğŸ“ æ¨¡å‹ä¿å­˜è·¯å¾„: {SAVE_PATH}")
print(f"ğŸ“„ æ•°æ®æ–‡ä»¶: {DATA_PATH}")

# ==================== æ£€æŸ¥æ–‡ä»¶ ====================
if not os.path.exists(DATA_PATH):
    raise FileNotFoundError(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {DATA_PATH}")

if not os.path.exists(BERT_LOCAL_PATH):
    raise FileNotFoundError(f"âŒ æœ¬åœ° BERT æ¨¡å‹ä¸å­˜åœ¨ï¼è¯·æ£€æŸ¥è·¯å¾„:\nğŸ‘‰ {BERT_LOCAL_PATH}")

required_files = ["pytorch_model.bin", "config.json", "vocab.txt"]
for f in required_files:
    if not os.path.exists(os.path.join(BERT_LOCAL_PATH, f)):
        raise FileNotFoundError(f"âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶: {f}ï¼Œè¯·ç¡®è®¤ BERT æ¨¡å‹å®Œæ•´")

# ==================== åŠ è½½æ•°æ® ====================
print("ğŸ“Š æ­£åœ¨åŠ è½½æ•°æ®...")
df = pd.read_csv(DATA_PATH)

if 'sentence' not in df.columns or 'label' not in df.columns:
    raise ValueError(f"âŒ CSV æ–‡ä»¶å¿…é¡»åŒ…å« 'sentence' å’Œ 'label' åˆ—ï¼ç°æœ‰åˆ—: {list(df.columns)}")

# ğŸ”¥ å¿«é€Ÿæ¨¡å¼ï¼šåªç”¨ 5 ä¸‡æ¡æ•°æ®ï¼ˆå¯æ³¨é‡Šæ‰è¿™è¡Œè·‘å…¨é‡ï¼‰
df = df.head(50000).reset_index(drop=True)
num_labels = df['label'].nunique()
print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± {len(df)} æ¡ï¼Œ{num_labels} ä¸ªç±»åˆ«")

# ==================== è‡ªå®šä¹‰ Dataset ====================
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=64):  # ğŸ”¥ max_len ä» 128 â†’ 64
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_len,
            return_tensors=None
        )

        return {
            'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),
            'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# ==================== åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹ ====================
print("ğŸ” æ­£åœ¨åŠ è½½æœ¬åœ° BERT æ¨¡å‹...")

try:
    tokenizer = BertTokenizer.from_pretrained(BERT_LOCAL_PATH)
    print("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")

    model = BertForSequenceClassification.from_pretrained(
        BERT_LOCAL_PATH,
        num_labels=num_labels
    )
    print(f"âœ… BERT æ¨¡å‹åŠ è½½æˆåŠŸï¼Œç±»åˆ«æ•°: {num_labels}")
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    raise

# ==================== è®­ç»ƒå‚æ•°ï¼ˆ4060 ä¼˜åŒ–ç‰ˆï¼‰====================
print("âš™ï¸  é…ç½®è®­ç»ƒå‚æ•°...")

training_args = TrainingArguments(
    output_dir=SAVE_PATH,
    num_train_epochs=1,                    # ğŸ”¥ åªè®­ç»ƒ 1 è½®ï¼Œè¶³å¤Ÿï¼
    per_device_train_batch_size=32,        # ğŸ”¥ 4060 æ”¯æŒå¤§ batchï¼ˆåŸ 16ï¼‰
    warmup_steps=200,                      # å°‘é‡ warmup
    weight_decay=0.01,
    logging_dir=os.path.join(SAVE_PATH, "logs"),
    logging_steps=50,                      # æ¯ 50 æ­¥è¾“å‡º loss
    save_steps=1000,
    save_total_limit=2,
    learning_rate=2e-5,
    seed=42,
    disable_tqdm=False,                    # æ˜¾ç¤ºè¿›åº¦æ¡
    report_to=[],                          # ä¸ä¸ŠæŠ¥
    fp16=True,                             # âš¡ å¯ç”¨åŠç²¾åº¦ï¼ˆ4060 æ”¯æŒï¼‰
    dataloader_num_workers=4,              # åŠ å¿«æ•°æ®åŠ è½½
    remove_unused_columns=True,
    # æ³¨æ„ï¼šæ—§ç‰ˆä¸æ”¯æŒ evaluation_strategy æˆ– device
)

# ==================== æ„å»ºè®­ç»ƒå™¨ ====================
print("ğŸ§  æ„å»ºè®­ç»ƒå™¨...")
try:
    train_dataset = TextDataset(
        df['sentence'].astype(str).tolist(),
        df['label'].tolist(),
        tokenizer,
        max_len=64  # ğŸ”¥ ä¸ä¸Šé¢ä¿æŒä¸€è‡´
    )
    print(f"âœ… æ•°æ®é›†æ„å»ºå®Œæˆï¼Œå…± {len(train_dataset)} ä¸ªæ ·æœ¬")

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
    )
    print("âœ… è®­ç»ƒå™¨æ„å»ºæˆåŠŸ")
except Exception as e:
    print(f"âŒ æ„å»ºè®­ç»ƒå™¨å¤±è´¥: {e}")
    raise
# ==================== å¼€å§‹è®­ç»ƒ ====================
if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹è®­ç»ƒ BERT æ¨¡å‹ï¼ˆ1 è½®ï¼Œæé€Ÿæ¨¡å¼ï¼‰...")
    print(f"ğŸ’¡ ä½¿ç”¨è®¾å¤‡: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")

    try:
        trainer.train()
        print("âœ… è®­ç»ƒå®Œæˆ")
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        raise

    # ==================== ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹ ====================
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹...")
    try:
        model.save_pretrained(SAVE_PATH)
        tokenizer.save_pretrained(SAVE_PATH)
        print(f"ğŸ‰ æ¨¡å‹å·²æˆåŠŸä¿å­˜è‡³:\nğŸ‘‰ {SAVE_PATH}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        raise

    # ==================== ä½¿ç”¨è¯´æ˜ ====================
    print("\n" + "="*60)
    print("ğŸ“Œ BERT æ¨¡å‹å¾®è°ƒå®Œæˆï¼ï¼ˆå¿«é€Ÿç‰ˆï¼‰")
    print("ğŸ’¡ åŠ è½½æ¨¡å‹æ–¹æ³•ï¼š")
    print(f"  model = BertForSequenceClassification.from_pretrained(r'{SAVE_PATH}')")
    print(f"  tokenizer = BertTokenizer.from_pretrained(r'{SAVE_PATH}')")
    print("="*60)