# main.py
#ç»ˆç«¯å¯åŠ¨ uvicorn main:app --reload
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from contextlib import asynccontextmanager
from typing import Dict, Any
import pickle
import os
import jieba
import numpy as np
import fasttext

# ------------------ è·¯å¾„é…ç½® ------------------
MODEL_DIR = r"D:\111huiyu\model"
STOPWORDS_PATH = r"D:\111huiyu\æ…§ä¸\è¯¾ä¸Šä»£ç \å¤´æ¡æ»¡åˆ†\data\stopwords.txt"

# ------------------ åŠ è½½åœç”¨è¯ ------------------
def load_stopwords(filepath: str) -> set:
    try:
        if os.path.exists(filepath):
            with open(filepath, 'r', encoding='utf-8') as f:
                return {line.strip() for line in f if line.strip()}
    except Exception as e:
        print(f"âš ï¸ è­¦å‘Šï¼šåŠ è½½åœç”¨è¯å¤±è´¥ {filepath}: {e}")
    return set()

stopwords = load_stopwords(STOPWORDS_PATH)

def preprocess_text(text: str) -> str:
    words = jieba.lcut(text)
    return ' '.join(w for w in words if w not in stopwords and len(w.strip()) > 1)

# ------------------ åˆ†ç±»æ ‡ç­¾ ------------------
CATEGORIES = {
    0: "finance",      1: "realty",     2: "stocks",
    3: "education",    4: "science",    5: "society",
    6: "politics",     7: "sports",     8: "game",
    9: "entertainment"
}

# ------------------ æ¨¡å‹æœåŠ¡ ------------------
class ModelService:
    def __init__(self):
        self.rf_model = None
        self.ft_model = None
        self.bert_model = None
        self.bert_tokenizer = None

    def load_models(self):
        """åŠ è½½æ‰€æœ‰æ¨¡å‹"""
        # 1. éšæœºæ£®æ—
        try:
            rf_path = os.path.join(MODEL_DIR, "rf_model.pkl")
            if not os.path.exists(rf_path):
                print(f"âŒ RF æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {rf_path}")
            else:
                with open(rf_path, "rb") as f:
                    self.rf_model = pickle.load(f)
                print("âœ… RF æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ RF åŠ è½½å¤±è´¥: {e}")

        # 2. fastText
        try:
            ft_path = os.path.join(MODEL_DIR, "fasttext_model.bin")
            if not os.path.exists(ft_path):
                print(f"âŒ fastText æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {ft_path}")
            else:
                self.ft_model = fasttext.load_model(ft_path)
                print("âœ… fastText æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ fastText åŠ è½½å¤±è´¥: {e}")

        # 3. BERT
        try:
            from transformers import BertForSequenceClassification, BertTokenizer
            bert_path = os.path.join(MODEL_DIR, "bert_model")
            if not os.path.exists(bert_path):
                print(f"âŒ BERT æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {bert_path}")
            else:
                self.bert_model = BertForSequenceClassification.from_pretrained(bert_path)
                self.bert_tokenizer = BertTokenizer.from_pretrained(bert_path)
                self.bert_model.eval()  # æ¨ç†æ¨¡å¼
                print("âœ… BERT æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ BERT åŠ è½½å¤±è´¥: {e}")

    def predict_rf(self, text: str) -> Dict[str, Any]:
        if not self.rf_model:
            return {"category": "unknown", "confidence": 0.0}
        try:
            cleaned = preprocess_text(text)
            pred = self.rf_model.predict([cleaned])[0]
            prob = self.rf_model.predict_proba([cleaned])[0].max()
            return {"category": CATEGORIES.get(pred, "unknown"), "confidence": float(prob)}
        except Exception as e:
            print(f"|RF é¢„æµ‹å¤±è´¥: {e}")
            return {"category": "unknown", "confidence": 0.0}

    def predict_fasttext(self, text: str) -> Dict[str, Any]:
        if not self.ft_model:
            return {"category": "unknown", "confidence": 0.0}
        try:
            cleaned = preprocess_text(text)
            if not cleaned or len(cleaned.strip()) == 0:
                print(f"âš ï¸ fastText è¾“å…¥ä¸ºç©º: '{text}' -> '{cleaned}'")
                return {"category": "unknown", "confidence": 0.0}

            cleaned = str(cleaned).strip()
            if not cleaned:
                return {"category": "unknown", "confidence": 0.0}

            print(f"fastText è¾“å…¥: '{cleaned}'")

            pred_label, pred_prob = self.ft_model.predict(cleaned)

            # âœ… ç¡®ä¿ label_str æ˜¯ str
            label_str = str(pred_label[0]) if isinstance(pred_label, (list, tuple)) and len(pred_label) > 0 else ""

            # âœ… å°† numpy æ¦‚ç‡è½¬ä¸º Python float
            conf = float(pred_prob[0]) if isinstance(pred_prob, (list, tuple, np.ndarray)) and len(
                pred_prob) > 0 else 0.0
            conf = float(conf)  # ç¡®ä¿æ˜¯ float

            # âœ… å…³é”®ä¿®å¤ï¼šè£å‰ªç½®ä¿¡åº¦åˆ° [0.0, 1.0]
            conf = max(0.0, min(1.0, conf))

            print(f"åŸå§‹ pred_prob ç±»å‹: {type(pred_prob[0])}, å€¼: {pred_prob[0]}")
            print(f"è£å‰ªå conf ç±»å‹: {type(conf)}, å€¼: {conf}")

            # è§£ææ ‡ç­¾
            if label_str.startswith('__label__'):
                try:
                    label = int(label_str.replace('__label__', ''))
                except ValueError:
                    label = -1
            else:
                label = -1

            category = CATEGORIES.get(label, "unknown")

            result = {"category": category, "confidence": conf}
            print(f"fastText è¾“å‡º: {result}")
            return result

        except Exception as e:
            print(f"âŒ fastText é¢„æµ‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {"category": "unknown", "confidence": 0.0}

    def predict_bert(self, text: str) -> Dict[str, Any]:
        if not self.bert_model or not self.bert_tokenizer:
            return {"category": "unknown", "confidence": 0.0}
        try:
            import torch
            inputs = self.bert_tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=128
            )
            with torch.no_grad():
                outputs = self.bert_model(**inputs)
                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
                conf, pred = torch.max(probs, dim=1)
                label = pred.item()
                return {"category": CATEGORIES.get(label, "unknown"), "confidence": conf.item()}
        except Exception as e:
            print(f"BERT é¢„æµ‹å¤±è´¥: {e}")
            return {"category": "unknown", "confidence": 0.0}


# ------------------ FastAPI åº”ç”¨ ------------------
@asynccontextmanager
async def lifespan(app: FastAPI):
    print("ğŸš€ æœåŠ¡å¯åŠ¨ä¸­...")
    model_service.load_models()
    print("âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆ")
    yield
    print("ğŸ›‘ æœåŠ¡å…³é—­")

app = FastAPI(title="å¤šæ¨¡å‹æ–‡æœ¬åˆ†ç±» API", lifespan=lifespan)

# CORS
from fastapi.middleware.cors import CORSMiddleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒè¯·æ”¹ä¸ºå…·ä½“åŸŸå
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ–æ¨¡å‹æœåŠ¡
model_service = ModelService()

# è¯·æ±‚ä½“
class TextRequest(BaseModel):
    text: str

# å“åº”ä½“ï¼ˆæ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼‰
class ModelResult(BaseModel):
    category: str
    confidence: float

# ç»¼åˆå“åº”
class MultiModelResponse(BaseModel):
    text: str
    random_forest: ModelResult
    fasttext: ModelResult
    bert: ModelResult

# ------------------ API æ¥å£ ------------------

@app.post("/predict", response_model=MultiModelResponse)
async def predict(request: TextRequest):
    text = request.text.strip()
    if not text:
        raise HTTPException(status_code=400, detail="æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

    try:
        result = MultiModelResponse(
            text=text,
            random_forest=model_service.predict_rf(text),
            fasttext=model_service.predict_fasttext(text),
            bert=model_service.predict_bert(text)
        )
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é¢„æµ‹å¤±è´¥: {str(e)}")

@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "models": {
            "random_forest": model_service.rf_model is not None,
            "fasttext": model_service.ft_model is not None,
            "bert": model_service.bert_model is not None
        },
        "message": "æœåŠ¡æ­£å¸¸è¿è¡Œ"
    }